# MANGO-LRQ Default Configuration
# Central YAML configuration for Memory-Adaptive Neural Gradient Optimizer

# Global experiment settings
experiment:
  name: "mango_lrq_experiment"
  output_dir: "./experiments/outputs"
  seed: 42
  device: "auto"  # auto, cuda, cpu
  distributed: false
  wandb_logging: true
  wandb_project: "mango-lrq"

# Model configuration
model:
  # Supported models: resnet18, resnet34, resnet50, vit_small, vit_base, bert_base, gpt2, llama-7b
  name: "resnet18"
  num_classes: 10  # Will be auto-detected based on dataset
  pretrained: false
  # Model-specific parameters
  dropout: 0.1
  activation: "relu"
  
# Dataset configuration  
dataset:
  # Supported: cifar10, cifar100, imagenet, alpaca, dolly, custom
  name: "cifar10"
  data_dir: "./data"
  batch_size: 128
  num_workers: 4
  pin_memory: true
  
  # Data augmentation
  augmentation:
    enabled: true
    horizontal_flip: true
    random_crop: true
    color_jitter: true
    normalize: true
    
  # Train/validation split
  train_split: 0.9
  val_split: 0.1
  max_train_samples: null  # null for full dataset
  max_val_samples: null

# Training configuration
training:
  # Basic training parameters
  epochs: 10
  learning_rate: 1e-3
  weight_decay: 1e-4
  momentum: 0.9
  
  # Learning rate schedule
  lr_scheduler:
    type: "cosine"  # constant, linear, cosine, exponential, step
    warmup_epochs: 2
    cosine_restarts: false
    step_size: 30  # for step scheduler
    gamma: 0.1     # for step/exponential scheduler
    
  # Gradient clipping
  gradient_clipping:
    enabled: true
    max_norm: 1.0
    norm_type: 2
    
  # Mixed precision training
  mixed_precision:
    enabled: true
    loss_scale: "dynamic"  # dynamic, static, or float value
    
  # Early stopping
  early_stopping:
    enabled: true
    patience: 5
    min_delta: 1e-4
    monitor: "val_loss"  # val_loss, val_acc
    
  # Checkpointing
  checkpointing:
    enabled: true
    save_best: true
    save_last: true
    save_every_n_epochs: 5
    monitor: "val_loss"

# MANGO-LRQ Optimizer Configuration
optimizer:
  # Optimizer selection
  type: "enhanced_mango"  # enhanced_mango, adamw, adafactor, sgd
  
  # Enhanced MANGO parameters
  enhanced_mango:
    use_mango_lrq: true
    use_tinyformer: true
    use_8bit_optimizer: true
    enable_amp: true
    enable_profiling: true
    profiler_output_dir: "./profiler_outputs"
    
    # MANGO-LRQ compression configuration
    compression:
      # Low-rank compression
      rank: 4
      adaptive_rank: true
      min_rank: 2
      max_rank: 32
      
      # Quantization settings
      bits_P: 4  # P matrix quantization bits
      bits_Q: 8  # Q matrix quantization bits
      momentum_precision: "fp16"  # fp16, fp32
      
      # Advanced compression features
      use_nf4: true
      error_feedback: true
      variance_reduction: true
      reference_steps: 10
      
      # Compression thresholds
      compression_threshold: 0.1
      error_threshold: 1e-3
      
    # TinyFormer policy network
    tinyformer:
      hidden_dim: 128
      num_layers: 6
      num_heads: 8
      dropout: 0.1
      activation: "gelu"
      
      # Policy learning
      policy_lr: 1e-4
      reward_weights:
        loss: 1.0
        memory: 0.1
        energy: 0.05
        time: 0.02
      
      # Feature extraction
      feature_dim: 35
      context_window: 10
      
  # Baseline optimizer configurations
  adamw:
    lr: 1e-3
    weight_decay: 1e-4
    betas: [0.9, 0.999]
    eps: 1e-8
    amsgrad: false
    
  adafactor:
    lr: null  # auto-scale
    weight_decay: 1e-4
    eps2: 1e-30
    cliping_threshold: 1.0
    decay_rate: -0.8
    beta1: null
    relative_step_size: true
    scale_parameter: true
    warmup_init: false
    
  sgd:
    lr: 1e-1
    momentum: 0.9
    weight_decay: 1e-4
    nesterov: true

# Monitoring and Profiling
monitoring:
  # Memory profiling
  memory_profiling:
    enabled: true
    interval_seconds: 1.0
    track_peak_memory: true
    track_memory_timeline: true
    export_reports: true
    
  # Power monitoring
  power_monitoring:
    enabled: true
    sampling_interval: 0.1
    enable_cpu_monitoring: true
    enable_gpu_monitoring: true
    export_csv: true
    
  # Multi-objective reward tracking
  multi_objective_reward:
    enabled: true
    baseline_update_frequency: 100
    reward_history_size: 1000
    
    # Reward function weights
    weights:
      loss: 1.0
      memory: 0.1
      energy: 0.05
      time: 0.02
    
  # Metrics to log
  metrics:
    log_every_n_steps: 50
    log_gradients: false
    log_parameters: false
    log_compression_stats: true
    log_memory_usage: true
    log_energy_consumption: true

# Distributed Training Configuration
distributed:
  # Backend selection
  backend: "fsdp"  # fsdp, deepspeed, ddp
  
  # FSDP configuration
  fsdp:
    sharding_strategy: "full_shard"  # full_shard, shard_grad_op, no_shard
    backward_prefetch: "backward_pre"
    mixed_precision: true
    cpu_offload: false
    auto_wrap_policy: "transformer"  # transformer, size_based, custom
    min_num_params: 1e6  # for size_based wrapping
    
  # DeepSpeed configuration  
  deepspeed:
    config_file: "configs/deepspeed_config.json"
    zero_stage: 3
    offload_optimizer: true
    offload_param: true
    
  # DDP configuration
  ddp:
    find_unused_parameters: false
    broadcast_buffers: true
    gradient_as_bucket_view: true

# Evaluation Configuration
evaluation:
  # Evaluation frequency
  eval_every_n_epochs: 1
  eval_every_n_steps: null
  
  # Metrics to compute
  metrics:
    - "accuracy"
    - "top5_accuracy"  # for classification
    - "loss"
    - "f1_score"
    - "precision"
    - "recall"
    
  # Evaluation datasets
  datasets:
    - "validation"
    - "test"  # if available
    
  # Model selection
  model_selection:
    metric: "val_accuracy"
    mode: "max"  # max, min
    
# Experiment-specific configurations
experiments:
  # Compression ratio sweep
  compression_sweep:
    enabled: false
    ranks: [2, 4, 8, 16, 32]
    bits_p: [2, 4, 8]
    bits_q: [4, 8, 16]
    
  # Ablation studies
  ablation:
    enabled: false
    components:
      - "nf4_quantization"
      - "error_feedback"  
      - "variance_reduction"
      - "tinyformer_policy"
      - "8bit_optimizer"
    
  # Baseline comparisons
  baseline_comparison:
    enabled: false
    optimizers:
      - "adamw"
      - "adafactor" 
      - "sgd"
      - "galore"
      - "ada_rank_grad"
    
  # Multi-model evaluation
  multi_model:
    enabled: false
    models:
      - "resnet18"
      - "resnet34"
      - "vit_small"
      - "bert_base"

# Hardware-specific optimizations
hardware:
  # GPU settings
  gpu:
    memory_fraction: 0.9  # Fraction of GPU memory to use
    allow_growth: true
    mixed_precision: true
    
  # CPU settings  
  cpu:
    num_threads: null  # null for auto-detection
    optimization: "O3"
    
  # Memory management
  memory:
    cache_cleanup_interval: 100  # steps
    garbage_collection_threshold: 0.8
    
# Debugging and Development
debug:
  # Debug modes
  enabled: false
  profile_memory: true
  profile_time: true
  check_gradients: true
  
  # Logging levels
  log_level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  verbose: false
  
  # Development flags
  fast_dev_run: false  # Run single batch for testing
  overfit_single_batch: false
  deterministic: false  # Make training deterministic (slower)

# Data collection for imitation learning  
data_collection:
  enabled: false
  output_dir: "./comp_ams_data"
  expert_type: "heuristic"  # heuristic, oracle, hybrid
  samples_per_model: 10000
  models:
    - "resnet18"
    - "vit_small" 
    - "bert_base"
  enable_visualization: true

# External integrations
integrations:
  # Weights & Biases
  wandb:
    entity: null  # your wandb entity
    tags: []
    notes: ""
    group: null
    
  # Tensorboard  
  tensorboard:
    enabled: false
    log_dir: "./tb_logs"
    
  # MLflow
  mlflow:
    enabled: false
    tracking_uri: null
    experiment_name: "mango_lrq"

# File paths and I/O
paths:
  # Data paths
  data_root: "./data"
  checkpoint_dir: "./checkpoints" 
  log_dir: "./logs"
  output_dir: "./outputs"
  
  # Model paths
  pretrained_model_path: null
  resume_from_checkpoint: null
  
  # Export paths
  export_model_path: null
  export_config_path: null

# Advanced features
advanced:
  # Gradient accumulation
  gradient_accumulation_steps: 1
  
  # Gradient checkpointing
  gradient_checkpointing: false
  
  # Model compilation
  torch_compile: false
  compile_mode: "default"  # default, reduce-overhead, max-autotune
  
  # Dynamic loss scaling
  dynamic_loss_scaling: true
  
  # Memory efficient attention
  memory_efficient_attention: false
  
  # Flash attention
  flash_attention: false

# Version and compatibility
version:
  config_version: "1.0"
  mango_version: "4.0"
  torch_min_version: "2.0.0"
  python_min_version: "3.8.0"