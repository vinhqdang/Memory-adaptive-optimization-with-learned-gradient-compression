"""
Power and Energy Monitoring for MANGO Optimizer

Multi-objective reward function including power consumption tracking
through nvidia-smi integration for comprehensive energy-aware optimization.
"""

import subprocess
import threading
import time
import queue
from typing import Dict, List, Optional, Tuple, Any
import torch
import numpy as np
from collections import deque
import logging
import psutil
import os


class PowerMonitor:
    """
    Real-time power monitoring using nvidia-smi for GPU power tracking.
    
    Provides energy-aware metrics for multi-objective optimization.
    """
    
    def __init__(
        self,
        sampling_interval: float = 0.1,  # 100ms sampling
        buffer_size: int = 1000,
        enable_cpu_monitoring: bool = True,
        enable_memory_monitoring: bool = True
    ):
        """
        Initialize power monitor.
        
        Args:
            sampling_interval: Time between power measurements (seconds)
            buffer_size: Number of samples to keep in buffer
            enable_cpu_monitoring: Monitor CPU power (approximate)
            enable_memory_monitoring: Monitor memory power consumption
        """
        self.sampling_interval = sampling_interval
        self.buffer_size = buffer_size
        self.enable_cpu_monitoring = enable_cpu_monitoring
        self.enable_memory_monitoring = enable_memory_monitoring
        
        # Power measurement buffers
        self.gpu_power_buffer = deque(maxlen=buffer_size)
        self.cpu_power_buffer = deque(maxlen=buffer_size)
        self.memory_power_buffer = deque(maxlen=buffer_size)
        self.timestamps = deque(maxlen=buffer_size)
        
        # Threading for async monitoring
        self.monitoring_thread = None
        self.power_queue = queue.Queue()
        self.is_monitoring = False
        self.lock = threading.Lock()
        
        # Power statistics
        self.total_energy_joules = 0.0
        self.start_time = None
        self.baseline_power = None
        
        # GPU device info
        self.gpu_count = torch.cuda.device_count() if torch.cuda.is_available() else 0
        
        # Logging setup
        self.logger = logging.getLogger(__name__)
    
    def start_monitoring(self) -> bool:\n        \"\"\"Start asynchronous power monitoring.\"\"\"\n        if self.is_monitoring:\n            self.logger.warning(\"Power monitoring already running\")\n            return True\n        \n        if self.gpu_count == 0:\n            self.logger.warning(\"No CUDA GPUs detected, GPU power monitoring disabled\")\n            return False\n        \n        # Check if nvidia-smi is available\n        try:\n            subprocess.run(['nvidia-smi', '--help'], \n                          capture_output=True, check=True, timeout=5)\n        except (subprocess.CalledProcessError, FileNotFoundError, subprocess.TimeoutExpired):\n            self.logger.error(\"nvidia-smi not available, cannot monitor GPU power\")\n            return False\n        \n        self.is_monitoring = True\n        self.start_time = time.time()\n        self.monitoring_thread = threading.Thread(target=self._monitoring_loop, daemon=True)\n        self.monitoring_thread.start()\n        \n        # Get baseline power consumption\n        time.sleep(0.5)  # Wait for initial measurements\n        self._establish_baseline()\n        \n        self.logger.info(f\"Power monitoring started with {self.gpu_count} GPUs\")\n        return True\n    \n    def stop_monitoring(self):\n        \"\"\"Stop power monitoring and cleanup.\"\"\"\n        self.is_monitoring = False\n        if self.monitoring_thread and self.monitoring_thread.is_alive():\n            self.monitoring_thread.join(timeout=2.0)\n        \n        self.logger.info(\"Power monitoring stopped\")\n    \n    def _monitoring_loop(self):\n        \"\"\"Main monitoring loop running in separate thread.\"\"\"\n        while self.is_monitoring:\n            try:\n                # Measure GPU power\n                gpu_power = self._measure_gpu_power()\n                \n                # Measure CPU power (approximate)\n                cpu_power = self._estimate_cpu_power() if self.enable_cpu_monitoring else 0.0\n                \n                # Measure memory power (approximate)\n                memory_power = self._estimate_memory_power() if self.enable_memory_monitoring else 0.0\n                \n                # Store measurements\n                timestamp = time.time()\n                with self.lock:\n                    self.gpu_power_buffer.append(gpu_power)\n                    self.cpu_power_buffer.append(cpu_power)\n                    self.memory_power_buffer.append(memory_power)\n                    self.timestamps.append(timestamp)\n                    \n                    # Update total energy consumption\n                    total_power = gpu_power + cpu_power + memory_power\n                    if len(self.timestamps) > 1:\n                        dt = timestamp - self.timestamps[-2]\n                        self.total_energy_joules += total_power * dt\n                \n                # Put sample in queue for real-time access\n                try:\n                    power_sample = {\n                        'timestamp': timestamp,\n                        'gpu_power_w': gpu_power,\n                        'cpu_power_w': cpu_power,\n                        'memory_power_w': memory_power,\n                        'total_power_w': total_power\n                    }\n                    self.power_queue.put_nowait(power_sample)\n                except queue.Full:\n                    # Drop oldest sample if queue is full\n                    try:\n                        self.power_queue.get_nowait()\n                        self.power_queue.put_nowait(power_sample)\n                    except queue.Empty:\n                        pass\n                \n                time.sleep(self.sampling_interval)\n                \n            except Exception as e:\n                self.logger.error(f\"Error in power monitoring: {e}\")\n                time.sleep(self.sampling_interval)\n    \n    def _measure_gpu_power(self) -> float:\n        \"\"\"Measure current GPU power consumption using nvidia-smi.\"\"\"\n        try:\n            cmd = [\n                'nvidia-smi',\n                '--query-gpu=power.draw',\n                '--format=csv,noheader,nounits'\n            ]\n            \n            result = subprocess.run(\n                cmd, capture_output=True, text=True, timeout=2.0\n            )\n            \n            if result.returncode != 0:\n                return 0.0\n            \n            # Parse power values for all GPUs\n            power_values = []\n            for line in result.stdout.strip().split('\\n'):\n                if line.strip():\n                    try:\n                        power = float(line.strip())\n                        power_values.append(power)\n                    except ValueError:\n                        continue\n            \n            return sum(power_values) if power_values else 0.0\n            \n        except (subprocess.TimeoutExpired, subprocess.CalledProcessError, Exception):\n            return 0.0\n    \n    def _estimate_cpu_power(self) -> float:\n        \"\"\"Estimate CPU power consumption based on utilization.\"\"\"\n        try:\n            # Get CPU utilization\n            cpu_percent = psutil.cpu_percent(interval=None)\n            \n            # Rough CPU power estimation (varies by CPU model)\n            # Typical desktop CPU: 65-125W TDP, server CPU: 150-250W TDP\n            estimated_tdp = 100.0  # Conservative estimate in watts\n            idle_power = estimated_tdp * 0.1  # ~10% at idle\n            \n            # Linear relationship between utilization and power\n            estimated_power = idle_power + (estimated_tdp - idle_power) * (cpu_percent / 100.0)\n            \n            return estimated_power\n            \n        except Exception:\n            return 0.0\n    \n    def _estimate_memory_power(self) -> float:\n        \"\"\"Estimate memory power consumption.\"\"\"\n        try:\n            # Get memory usage\n            memory = psutil.virtual_memory()\n            memory_gb = memory.total / (1024**3)\n            memory_usage_percent = memory.percent\n            \n            # Rough memory power estimation\n            # DDR4: ~3-5W per 8GB module\n            power_per_gb = 0.5  # Watts per GB\n            base_power = memory_gb * power_per_gb * 0.2  # Base power\n            usage_power = memory_gb * power_per_gb * 0.8 * (memory_usage_percent / 100.0)\n            \n            return base_power + usage_power\n            \n        except Exception:\n            return 0.0\n    \n    def _establish_baseline(self):\n        \"\"\"Establish baseline power consumption.\"\"\"\n        if len(self.gpu_power_buffer) >= 5:\n            with self.lock:\n                recent_samples = list(self.gpu_power_buffer)[-5:]\n                self.baseline_power = np.mean(recent_samples)\n    \n    def get_current_power(self) -> Dict[str, float]:\n        \"\"\"Get current power consumption.\"\"\"\n        try:\n            power_sample = self.power_queue.get_nowait()\n            return power_sample\n        except queue.Empty:\n            with self.lock:\n                if self.gpu_power_buffer and self.cpu_power_buffer and self.memory_power_buffer:\n                    return {\n                        'timestamp': time.time(),\n                        'gpu_power_w': self.gpu_power_buffer[-1],\n                        'cpu_power_w': self.cpu_power_buffer[-1],\n                        'memory_power_w': self.memory_power_buffer[-1],\n                        'total_power_w': (self.gpu_power_buffer[-1] + \n                                        self.cpu_power_buffer[-1] + \n                                        self.memory_power_buffer[-1])\n                    }\n                else:\n                    return {\n                        'timestamp': time.time(),\n                        'gpu_power_w': 0.0,\n                        'cpu_power_w': 0.0,\n                        'memory_power_w': 0.0,\n                        'total_power_w': 0.0\n                    }\n    \n    def get_power_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get comprehensive power statistics.\"\"\"\n        with self.lock:\n            if not self.gpu_power_buffer:\n                return {'error': 'No power data available'}\n            \n            gpu_powers = np.array(list(self.gpu_power_buffer))\n            cpu_powers = np.array(list(self.cpu_power_buffer)) if self.cpu_power_buffer else np.array([0])\n            memory_powers = np.array(list(self.memory_power_buffer)) if self.memory_power_buffer else np.array([0])\n            total_powers = gpu_powers + cpu_powers + memory_powers\n            \n            elapsed_time = time.time() - self.start_time if self.start_time else 0\n            \n            stats = {\n                'gpu_power': {\n                    'current_w': float(gpu_powers[-1]) if len(gpu_powers) > 0 else 0.0,\n                    'mean_w': float(np.mean(gpu_powers)),\n                    'max_w': float(np.max(gpu_powers)),\n                    'min_w': float(np.min(gpu_powers)),\n                    'std_w': float(np.std(gpu_powers))\n                },\n                'total_power': {\n                    'current_w': float(total_powers[-1]) if len(total_powers) > 0 else 0.0,\n                    'mean_w': float(np.mean(total_powers)),\n                    'max_w': float(np.max(total_powers)),\n                    'min_w': float(np.min(total_powers))\n                },\n                'energy': {\n                    'total_joules': self.total_energy_joules,\n                    'total_kwh': self.total_energy_joules / 3.6e6,  # Convert to kWh\n                    'average_power_w': self.total_energy_joules / max(elapsed_time, 1.0)\n                },\n                'monitoring': {\n                    'elapsed_time_s': elapsed_time,\n                    'samples_collected': len(gpu_powers),\n                    'sampling_rate_hz': len(gpu_powers) / max(elapsed_time, 1.0),\n                    'baseline_power_w': self.baseline_power if self.baseline_power else 0.0\n                }\n            }\n            \n            return stats\n    \n    def get_energy_efficiency_metric(self, training_loss: float) -> float:\n        \"\"\"Calculate energy efficiency metric for multi-objective optimization.\"\"\"\n        stats = self.get_power_statistics()\n        \n        if 'energy' not in stats:\n            return 0.0\n        \n        total_energy = stats['energy']['total_joules']\n        \n        if total_energy == 0 or training_loss == 0:\n            return 0.0\n        \n        # Energy efficiency: lower is better (energy per unit loss reduction)\n        # Normalize by baseline energy consumption\n        baseline_energy = (self.baseline_power or 100.0) * stats['monitoring']['elapsed_time_s']\n        excess_energy = max(0, total_energy - baseline_energy)\n        \n        # Efficiency metric: penalize high energy consumption\n        efficiency = 1.0 / (1.0 + excess_energy / max(training_loss, 1e-6))\n        \n        return efficiency\n    \n    def export_power_data(self, filename: str) -> bool:\n        \"\"\"Export power monitoring data to CSV file.\"\"\"\n        try:\n            with self.lock:\n                if not self.gpu_power_buffer:\n                    return False\n                \n                import csv\n                \n                with open(filename, 'w', newline='') as csvfile:\n                    writer = csv.writer(csvfile)\n                    writer.writerow([\n                        'timestamp', 'gpu_power_w', 'cpu_power_w', \n                        'memory_power_w', 'total_power_w'\n                    ])\n                    \n                    for i in range(len(self.timestamps)):\n                        writer.writerow([\n                            self.timestamps[i],\n                            self.gpu_power_buffer[i],\n                            self.cpu_power_buffer[i] if i < len(self.cpu_power_buffer) else 0,\n                            self.memory_power_buffer[i] if i < len(self.memory_power_buffer) else 0,\n                            (self.gpu_power_buffer[i] + \n                             (self.cpu_power_buffer[i] if i < len(self.cpu_power_buffer) else 0) +\n                             (self.memory_power_buffer[i] if i < len(self.memory_power_buffer) else 0))\n                        ])\n                \n                self.logger.info(f\"Power data exported to {filename}\")\n                return True\n                \n        except Exception as e:\n            self.logger.error(f\"Failed to export power data: {e}\")\n            return False\n    \n    def __enter__(self):\n        \"\"\"Context manager entry.\"\"\"\n        self.start_monitoring()\n        return self\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit.\"\"\"\n        self.stop_monitoring()\n\n\nclass MultiObjectiveReward:\n    \"\"\"Multi-objective reward function for energy-aware optimization.\"\"\"\n    \n    def __init__(\n        self,\n        power_monitor: Optional[PowerMonitor] = None,\n        loss_weight: float = 1.0,\n        memory_weight: float = 0.1,\n        energy_weight: float = 0.05,\n        time_weight: float = 0.02\n    ):\n        \"\"\"Initialize multi-objective reward.\"\"\"\n        self.power_monitor = power_monitor\n        self.loss_weight = loss_weight\n        self.memory_weight = memory_weight\n        self.energy_weight = energy_weight\n        self.time_weight = time_weight\n        \n        # Baseline values for normalization\n        self.baseline_loss = None\n        self.baseline_memory = None\n        self.baseline_energy = None\n        self.baseline_time = None\n    \n    def compute_reward(\n        self,\n        current_loss: float,\n        memory_usage_gb: float,\n        training_time_s: float\n    ) -> Tuple[float, Dict[str, float]]:\n        \"\"\"\n        Compute multi-objective reward.\n        \n        Args:\n            current_loss: Current training loss\n            memory_usage_gb: Current memory usage in GB\n            training_time_s: Training time in seconds\n            \n        Returns:\n            Tuple of (total_reward, component_rewards)\n        \"\"\"\n        rewards = {}\n        \n        # Loss reward (negative loss, higher is better)\n        if self.baseline_loss is None:\n            self.baseline_loss = current_loss\n        \n        loss_improvement = max(0, self.baseline_loss - current_loss)\n        rewards['loss'] = self.loss_weight * loss_improvement\n        \n        # Memory efficiency reward (lower memory usage is better)\n        if self.baseline_memory is None:\n            self.baseline_memory = memory_usage_gb\n        \n        memory_efficiency = max(0, self.baseline_memory - memory_usage_gb) / max(self.baseline_memory, 1.0)\n        rewards['memory'] = self.memory_weight * memory_efficiency\n        \n        # Energy efficiency reward\n        if self.power_monitor:\n            energy_efficiency = self.power_monitor.get_energy_efficiency_metric(current_loss)\n            rewards['energy'] = self.energy_weight * energy_efficiency\n        else:\n            rewards['energy'] = 0.0\n        \n        # Time efficiency reward (faster training is better)\n        if self.baseline_time is None:\n            self.baseline_time = training_time_s\n        \n        if training_time_s > 0:\n            time_efficiency = self.baseline_time / max(training_time_s, 1e-6)\n            rewards['time'] = self.time_weight * min(1.0, time_efficiency)\n        else:\n            rewards['time'] = 0.0\n        \n        # Total reward\n        total_reward = sum(rewards.values())\n        \n        return total_reward, rewards\n    \n    def update_baselines(\n        self,\n        loss: float,\n        memory_gb: float,\n        time_s: float\n    ):\n        \"\"\"Update baseline values for normalization.\"\"\"\n        alpha = 0.1  # Exponential moving average factor\n        \n        if self.baseline_loss is None:\n            self.baseline_loss = loss\n        else:\n            self.baseline_loss = (1 - alpha) * self.baseline_loss + alpha * loss\n        \n        if self.baseline_memory is None:\n            self.baseline_memory = memory_gb\n        else:\n            self.baseline_memory = (1 - alpha) * self.baseline_memory + alpha * memory_gb\n        \n        if self.baseline_time is None:\n            self.baseline_time = time_s\n        else:\n            self.baseline_time = (1 - alpha) * self.baseline_time + alpha * time_s